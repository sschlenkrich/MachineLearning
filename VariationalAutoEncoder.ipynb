{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "In this notebook we implement a basic test of an Variational Autoencoder (VAE). The variational autoencoder is used to learn the distribution of yield curves.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "\n",
    "  - Generate input yield curves from a Hull White model.\n",
    "  - Setup a VAE based on the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/generative/cvae)..\n",
    "  - Train the VAE to the Hull White yield curves and test the model.\n",
    "  - Save and plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hull White Yield Curves\n",
    "\n",
    "We model yield curves in terms of *continuous compounded zero rates*. A zero rate yield curve is a function $z:[0,\\infty)\\times[0,\\infty) \\rightarrow \\mathbb{R}$. For a given observation time $t\\geq 0$ and maturity time $T\\geq t$ the zero rate $z(t,T)$ gives a zero coupon bond price (or discount factor) $P(t,T)$ via the relation\n",
    "$$\n",
    "  P(t,T) = e^{-z(t,T)(T-t)}.\n",
    "$$\n",
    "Equivalently, we can calculate the zero rate from a zero coupon bond price as\n",
    "$$\n",
    "  z(t,T) = - \\frac{\\log\\left( P(t,T) \\right)}{T-t}.\n",
    "$$\n",
    "\n",
    "In Hull White model the zero bond prices can be reconstructed from a Gaussian state variable $x_t$ and\n",
    "$$\n",
    "  P(t,T) = \\frac{P(0,T)}{P(0,t)} e^{-G(t,T)x_t - \\frac{1}{2}G(t,T)^2y(t)}.\n",
    "$$\n",
    "Here, $G(t,T)$ and $y(t)$ are model functions given as\n",
    "$$\n",
    "  G(t,T) = \\frac{1}{a}\\left[1 - e^{-a(T-t)}\\right]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "  y(t) = \\int_0^t \\left[e^{-a(t-u)} \\sigma(u) \\right]^2 du =  \\frac{1}{2a}\\left[1 - e^{-2at}\\right]\\sigma^2.\n",
    "$$\n",
    "Model parameters are mean reversion $a$ and short rate volatility $\\sigma(t)=\\sigma$.\n",
    "\n",
    "Consequently, yield curves can be represented as\n",
    "$$\n",
    "  z(t,T) = \\frac{G(t,T)}{T-t} x_t + \\frac{1}{2} \\frac{G(t,T)^2y(t)}{T-t} + \\left[ z(0,T) - z(0,t) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HullWhiteModel:\n",
    "\n",
    "    def __init__(self, mean_reversion, volatility, zeroYieldCurve=None):\n",
    "        self.mean_reversion = mean_reversion\n",
    "        self.volatility = volatility\n",
    "        self.zeroYieldCurve = zeroYieldCurve\n",
    "\n",
    "    def G(self, t,T): \n",
    "        return (1 - np.exp(-self.mean_reversion*(T-t))) / self.mean_reversion\n",
    "    \n",
    "    def y(self, t):\n",
    "        return self.volatility**2 * (1 - np.exp(-2*self.mean_reversion*(t))) / 2 / self.mean_reversion\n",
    "\n",
    "    def zeroRate(self, x, t, T):\n",
    "        G = self.G(t,T)\n",
    "        z = G / (T-t) * x + 0.5 * G**2 * self.y(t) / (T-t)\n",
    "        if self.zeroYieldCurve is not None:\n",
    "            z += self.zeroYieldCurve(T) - self.zeroYieldCurve(t)\n",
    "        return z\n",
    "\n",
    "    def yieldCurves(self, t, delta_T, num_samples):\n",
    "        \"\"\"\n",
    "        Simulate yield curves from 0 to t using the model parameters.\n",
    "\n",
    "        State variables are simulated in t-forward measure.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        t        ... future observation time\n",
    "        delta_T  ... array of time offsets to calculate z(t, t + delta_T)\n",
    "        num_samples  ... number of yield curve samples calculated\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        A an array of shape (num_samples, len(delta_T)) containing\n",
    "        simulated zero rates z(t, t + delta_T).\n",
    "        \"\"\"\n",
    "        x = np.random.normal(size=(num_samples,1)) * np.sqrt(self.y(t))\n",
    "        return self.zeroRate(x, t, t + delta_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a utility function to consistently plot curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_yieldCurves(curves, N=10):\n",
    "    plt.Figure(figsize=(6,4))\n",
    "    N = np.minimum(N, curves.shape[0])\n",
    "    for yc in curves[:N]:\n",
    "        plt.plot(delta_T, yc)\n",
    "    plt.xlabel('maturity times $T-t$')\n",
    "    plt.ylabel('zero rate $z(t,T)$')\n",
    "    plt.title('simulated curves for $t=%.1f$ ($a=%.1f$%%, $\\sigma=%.1f$bp)' % (t, model.mean_reversion*1e2, model.volatility*1e4))\n",
    "    plt.show()\n",
    "    #\n",
    "    print('Shape: ' + str(curves.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a simple Hull White model and simulate future yield curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HullWhiteModel(0.15, 0.0075)  # 15% mean reversion (fairly high) and 75bp vol\n",
    "t = 10.0  # horizon in 10y\n",
    "delta_T = np.array([1.0/365, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 15.0, 20.0])  # a typical curve grid \n",
    "\n",
    "num_samples = 2**10\n",
    "\n",
    "yieldCurves = model.yieldCurves(10.0, delta_T, num_samples)\n",
    "\n",
    "plot_yieldCurves(yieldCurves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder using Keras\n",
    "\n",
    "We setup a VAE implementation using Keras, see [TensorFlow tutorial](https://www.tensorflow.org/tutorials/generative/cvae)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(tf.keras.Model):\n",
    "    \"\"\"A variational autoencoder as a Keras model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, alpha=0.01):\n",
    "        super().__init__()\n",
    "        self.input_dim  = input_dim   # number of inputs and outputs flattened as vector \n",
    "        self.hidden_dim = hidden_dim  # number of hidden nodes\n",
    "        self.latent_dim = latent_dim  # number of latent variables, i.e. dimensionality of latent space\n",
    "        self.alpha      = alpha       # convex combination of to minimize reconstruction (0) or latent distribution (1)\n",
    "        #\n",
    "        lrelu = tf.keras.layers.LeakyReLU(alpha=0.3)  # functor for activation function\n",
    "        #\n",
    "        self.encoder = tf.keras.Sequential( [\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.input_dim)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation=lrelu),\n",
    "            tf.keras.layers.Dense(2 * self.latent_dim, activation=lrelu),  # mu, logvar\n",
    "        ] )\n",
    "        self.decoder = tf.keras.Sequential( [\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation=lrelu),\n",
    "            tf.keras.layers.Dense(self.input_dim, activation=tf.keras.activations.linear),\n",
    "        ] )\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "  \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "  \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Specify model output calculation for training.\n",
    "\n",
    "        This function is overloaded from tf.keras.Model.\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(inputs)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_out = self.decode(z)\n",
    "        return tf.concat([x_out, mean, logvar], axis=1)\n",
    "\n",
    "    def lossfunction(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Specify the objective function for optimisation.\n",
    "\n",
    "        This function is input to tf.keras.Model.compile(...)\n",
    "        \"\"\"\n",
    "        y = tf.cast(y_true, tf.float32)\n",
    "        x_out  = y_pred[:, : -2*self.latent_dim                  ]\n",
    "        mean   = y_pred[:, -2*self.latent_dim : -self.latent_dim ]\n",
    "        logvar = y_pred[:, -self.latent_dim :                    ]\n",
    "        #\n",
    "        decoded_loss = tf.reduce_sum(tf.math.squared_difference(x_out, y), 1)\n",
    "        latent_loss = 0.5 * tf.reduce_sum(tf.exp(logvar) + tf.square(mean)  - 1. - logvar, 1)  # https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions\n",
    "        loss = tf.reduce_mean((1 - self.alpha) * decoded_loss + self.alpha * latent_loss)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, n_samples = 10, randoms=None):\n",
    "        \"\"\"\n",
    "        Calculate a sample of observations from the model.\n",
    "        \"\"\"\n",
    "        if randoms is None: # we do need to sample\n",
    "            randoms = tf.random.normal(shape=(n_samples, self.latent_dim))\n",
    "        return self.decode(randoms)\n",
    "\n",
    "    def functional_model(self):\n",
    "        \"\"\"\n",
    "        Return a standard tf.keras.Model via Functional API.\n",
    "\n",
    "        The resulting model can be used to plot the architecture.\n",
    "        \"\"\"\n",
    "        x = tf.keras.Input(shape=(self.input_dim))\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Taining and Testing\n",
    "\n",
    "Now, we can setup a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VariationalAutoencoder(input_dim=yieldCurves.shape[1], hidden_dim=yieldCurves.shape[1], latent_dim=1, alpha=0.5*1e-4)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "vae_model.compile(optimizer=optimizer, loss=vae_model.lossfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using the curves generated from the (analytic) Hull White model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.fit(x=yieldCurves, y=yieldCurves, epochs=100, callbacks=[TqdmCallback(verbose=0)], verbose=0)\n",
    "yieldCurves_vae2 = vae_model.sample(10)\n",
    "plot_yieldCurves(yieldCurves_vae2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Plot a Model\n",
    "\n",
    "In this section we explore functionality to save and plot Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = 'HullWhiteCurveVae'\n",
    "\n",
    "vae_model.save(model_folder_name)\n",
    "#\n",
    "reconstructed_model = tf.keras.models.load_model(model_folder_name,\n",
    "    custom_objects={\n",
    "        'VariationalAutoencoder': VariationalAutoencoder,\n",
    "        'lossfunction' : VariationalAutoencoder.lossfunction,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loose the custom functions like *VariationalAutoencoder.sample(...)* in the reconstructed model. Nevertheless, we can still access the attributes. And the *decoder* is all we need to generate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, latent_dim, n_samples = 10, randoms=None):\n",
    "    \"\"\"\n",
    "    Calculate a sample of observations from the model.\n",
    "    \"\"\"\n",
    "    if randoms is None: # we do need to sample\n",
    "        randoms = tf.random.normal(shape=(n_samples, latent_dim))\n",
    "    return model.decoder(randoms)\n",
    "\n",
    "plot_yieldCurves(sample(reconstructed_model, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    vae_model.functional_model(),\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=False,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional VAE\n",
    "\n",
    "We extend the VAE by adding external conditions. This follows the ideas presented in [GitHub:MarketSimulator](https://github.com/imanolperez/market_simulator).\n",
    "\n",
    "In our yield curve example the external condition is *time-to-maturity*. That is, instead of a yield curve as vector, we now learn a yield curve functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVariationalAutoencoder(tf.keras.Model):\n",
    "    \"\"\"Conditional variational autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim, alpha=0.01):\n",
    "        super().__init__()\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.alpha      = alpha\n",
    "        #\n",
    "        lrelu = tf.keras.layers.LeakyReLU(alpha=0.3)  # functor for activation function\n",
    "        #\n",
    "        self.encoder = tf.keras.Sequential( [\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.input_dim)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation=lrelu),\n",
    "            tf.keras.layers.Dense(2 * self.latent_dim, activation=lrelu),  # mu, logvar\n",
    "        ] )\n",
    "        self.decoder = tf.keras.Sequential( [\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim + self.input_dim - self.output_dim)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation=lrelu),\n",
    "            tf.keras.layers.Dense(self.output_dim, activation=tf.keras.activations.linear),\n",
    "        ] )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        x = tf.concat([x, c], axis=1)\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "  \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "  \n",
    "    def decode(self, z, c):\n",
    "        z = tf.concat([z, c], axis=1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        assert isinstance(inputs, (list, tuple))\n",
    "        assert len(inputs)==2\n",
    "        x = inputs[0]\n",
    "        c = inputs[1]\n",
    "        mean, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_out = self.decode(z, c)\n",
    "        return tf.concat([x_out, mean, logvar], axis=1)\n",
    "\n",
    "    def lossfunction(self, y_true, y_pred, sample_weight=None):\n",
    "        y = tf.cast(y_true, tf.float32)\n",
    "        x_out  = y_pred[:, : -2*self.latent_dim                  ]\n",
    "        mean   = y_pred[:, -2*self.latent_dim : -self.latent_dim ]\n",
    "        logvar = y_pred[:, -self.latent_dim :                    ]\n",
    "        #\n",
    "        decoded_loss = tf.reduce_sum(tf.math.squared_difference(x_out, y), 1)\n",
    "        latent_loss = 0.5 * tf.reduce_sum(tf.exp(logvar) + tf.square(mean)  - 1. - logvar, 1)  # https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions\n",
    "        loss = tf.reduce_mean((1 - self.alpha) * decoded_loss + self.alpha * latent_loss)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, n_samples, c, randoms=None):\n",
    "        if randoms is None: # we do need to sample\n",
    "            randoms = tf.random.normal(shape=(n_samples, self.latent_dim))\n",
    "        # we need the Cartesian product of randoms and conditions\n",
    "        z_full = tf.concat([randoms for row in c], axis=0)\n",
    "        zero = np.zeros(shape=(randoms.shape[0], c.shape[1]))\n",
    "        c_full = tf.concat([ tf.cast(zero+row, tf.float32) for row in c], axis=0)\n",
    "        dec_outputs = self.decode(z_full, c_full)\n",
    "        #return tf.reshape(dec_outputs, shape=(randoms.shape[0],c.shape[0]))\n",
    "        return \\\n",
    "            tf.transpose(tf.reshape(dec_outputs, shape=(c.shape[0],randoms.shape[0]))), \\\n",
    "            tf.transpose(tf.reshape(c_full,      shape=(c.shape[0],randoms.shape[0])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element of your yield curves we specify the time-to-maturity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = np.zeros(shape=yieldCurves.shape) + delta_T\n",
    "condition[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VAE accepts inputs as vectors. Since we want to model individual yield curve values, we need to flatten curve values and time-to-maturity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = yieldCurves.flatten()\n",
    "x.shape = (x.shape[0], 1)\n",
    "print(x.shape)\n",
    "c = condition.flatten()\n",
    "c.shape = (c.shape[0], 1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VAE model has two inputs: curve value and time-to-maturity. As output we only have one quantity: curve value. We also put a lot of emphasis on re-constructions. Thus $\\alpha$ is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_model = ConditionalVariationalAutoencoder(input_dim=2, hidden_dim=yieldCurves.shape[1], latent_dim=1, output_dim=1, alpha=0.5*1e-4)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "cvae_model.compile(optimizer=optimizer, loss=vae_model.lossfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sample calculation we need to supply the condition (i.e. time-to-maturity) as a row of the condition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_model.fit(x=(x,c), y=x, epochs=1000, callbacks=[TqdmCallback(verbose=0)], verbose=0)\n",
    "#\n",
    "cond = np.reshape(delta_T, (delta_T.shape[0],1))\n",
    "yieldCurves_cvae, delta_T_s = cvae_model.sample(8, c=cond)\n",
    "plot_yieldCurves(yieldCurves_cvae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, the fit is not as good as when we learn the full shape of the curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also verify that the data transformations worked out by inspecting the equally re-shaped condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_T_s[1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af98fb13c0682ca4fe3350401d42f2a404b34a8b53a566210d6d775d501366cd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('Python3.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
